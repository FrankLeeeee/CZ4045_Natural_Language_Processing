{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CCONJ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keep punctuation: The service is fast, room is clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NounAndAdjPair:\n",
    "    \n",
    "    patterns=['(ADJ )*(NOUN )+(ADV )*(VERB )+(ADV )*ADJ ', # The Korean grill is good\n",
    "              '(NOUN )+.*PRON (VERB )+(ADV )*ADJ ', \n",
    "              '(ADV )*(ADJ )+(NOUN )+', # good service\n",
    "             ]\n",
    "    \n",
    "    def __init__(self, doc):\n",
    "        \n",
    "        def getDoc():\n",
    "            return re.sub(' +', ' ', self.original_doc.lower().replace('\\n', ' ').strip()).lower()#.replace('soooo', 'so') 6\n",
    "\n",
    "        def getSentences():\n",
    "            sentences = [s for s in sent_tokenize(self.doc)] # nltk sentence tokenizer\n",
    "            return [' '.join([w.text for w in nlp(s)]) for s in sentences] # spacy word tokenizer\n",
    "\n",
    "        def getTaggings():\n",
    "            return [' '.join([w.pos_ for w in nlp(s)])+' ' for s in self.sentences]  \n",
    "        \n",
    "        self.original_doc = doc\n",
    "        self.doc = getDoc()\n",
    "        self.sentences = getSentences()\n",
    "        self.taggings = getTaggings()\n",
    "#         print(len(self.sentences), len(self.taggings))\n",
    "\n",
    "    def getPairsWithFSA(self, returnInDf=True):\n",
    "        \n",
    "        def checkIfAdjAndNounExists(tagging):\n",
    "            if 'ADJ' in tagging and 'NOUN' in tagging:\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def getWoldsByTaggingIndex(target_tagging_index, no_sentence, no_pattern):\n",
    "            \n",
    "            original_tagging = self.taggings[no_sentence]\n",
    "            \n",
    "            baseIndex = 0 if target_tagging_index[0]==0 else len(original_tagging[:target_tagging_index[0]].strip().split(' '))\n",
    "            buildIndex = len(original_tagging[target_tagging_index[0]:target_tagging_index[1]].strip().split(' '))\n",
    "            \n",
    "            trimmed_tagging_list = original_tagging.strip().split(' ')[baseIndex: baseIndex+buildIndex]\n",
    "            trimmed_sentence_list = self.sentences[no_sentence].split(' ')[baseIndex: baseIndex+buildIndex]\n",
    "            \n",
    "            \n",
    "            noun_adj_pair = []\n",
    "            \n",
    "#             print(trimmed_sentence_list, '\\n', trimmed_tagging_list)\n",
    "            \n",
    "            if no_pattern==0: # the first pattern\n",
    "                last_verb_index = len(trimmed_tagging_list)-trimmed_tagging_list[::-1].index('VERB')-1\n",
    "                for i in range(len(trimmed_tagging_list)):\n",
    "                    if trimmed_tagging_list[i] not in ['ADJ', 'NOUN']: #filter out adv\n",
    "                        break\n",
    "                        \n",
    "                noun_adj_pair.append(' '.join(trimmed_sentence_list[:i]))\n",
    "                noun_adj_pair.append(' '.join(trimmed_sentence_list[last_verb_index+1:]))\n",
    "                \n",
    "            elif no_pattern==1:\n",
    "                last_noun_end_index = len(trimmed_tagging_list)-trimmed_tagging_list[::-1].index('NOUN')-1\n",
    "                for i in range(last_noun_end_index, 0, -1):\n",
    "                    if trimmed_tagging_list[i]=='NOUN':\n",
    "                        last_noun_start_index = i\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "                last_verb_index = len(trimmed_tagging_list)-trimmed_tagging_list[::-1].index('VERB')-1\n",
    "                        \n",
    "                noun_adj_pair.append(' '.join(trimmed_sentence_list[last_noun_start_index:last_noun_end_index+1]))\n",
    "                noun_adj_pair.append(' '.join(trimmed_sentence_list[last_verb_index+1:]))\n",
    "            \n",
    "            elif no_pattern==2:\n",
    "                noun_index = trimmed_tagging_list.index('NOUN')\n",
    "                noun_adj_pair.append(' '.join(trimmed_sentence_list[noun_index:]))\n",
    "                noun_adj_pair.append(' '.join(trimmed_sentence_list[:noun_index]))\n",
    "                \n",
    "            \n",
    "            return tuple(noun_adj_pair)\n",
    "        \n",
    "        \n",
    "        def formDf(sentence_adjNoun_pair):\n",
    "            df = pd.DataFrame(columns=['Sentence', 'AdjNounPair'])\n",
    "            pd.set_option('display.max_colwidth', -1)\n",
    "            df.Sentence, df.AdjNounPair = list(sentence_adjNoun_pair.keys()), list(sentence_adjNoun_pair.values())\n",
    "            return df\n",
    "        \n",
    "        sentence_adjNoun_pair = dict(zip(self.sentences, [[] for i in range(len(self.sentences))]))\n",
    "        \n",
    "        for i in range(len(self.sentences)):\n",
    "            s = self.sentences[i]\n",
    "            t = self.taggings[i]\n",
    "            \n",
    "            if checkIfAdjAndNounExists(t):\n",
    "                for p in self.patterns:\n",
    "                    for x in re.finditer(p, t):\n",
    "                        sentence_adjNoun_pair[s].append(getWoldsByTaggingIndex(x.span(), i, self.patterns.index(p)))\n",
    "                        \n",
    "                        \n",
    "        if returnInDf:\n",
    "            return formDf(sentence_adjNoun_pair)\n",
    "            \n",
    "        return sentence_adjNoun_pair\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = NounAndAdjPair(df.text[2])\n",
    "a.getPairsWithFSA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
