{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUNNING RESULT FROM PY FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------------------- Selecting 5 business ID --------------------\n",
      "\n",
      "Business ID:  8Z72HW5ydzQFydUxZglurg\n",
      "Business ID:  QeEQXdto_4wFRaNKyIygRA\n",
      "Business ID:  IUMyUYOIR9UQ7XGIEQKOuA\n",
      "Business ID:  Rii85bzYKGC9P0zOyAem6A\n",
      "Business ID:  i-2OzvZUDtvKCMq1vcRSZg\n",
      "\n",
      "------------------------- Getting pairs -------------------------\n",
      "\n",
      "Processing reviews from business ID: 8Z72HW5ydzQFydUxZglurg ...\n",
      "Processing reviews from business ID: QeEQXdto_4wFRaNKyIygRA ...\n",
      "Processing reviews from business ID: IUMyUYOIR9UQ7XGIEQKOuA ...\n",
      "Processing reviews from business ID: Rii85bzYKGC9P0zOyAem6A ...\n",
      "Processing reviews from business ID: i-2OzvZUDtvKCMq1vcRSZg ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BusinessId</th>\n",
       "      <th>AdjNounPair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8Z72HW5ydzQFydUxZglurg</td>\n",
       "      <td>[((food, good), 12), ((buffet, chinese), 10), ((bar, sushi), 8), ((staff, friendly), 6), ((buffets, chinese), 6)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QeEQXdto_4wFRaNKyIygRA</td>\n",
       "      <td>[((quarter, french), 29), ((food, good), 15), ((cristo, monte), 11), ((service, great), 8), ((service, good), 6)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IUMyUYOIR9UQ7XGIEQKOuA</td>\n",
       "      <td>[((food, good), 8), ((service, slow), 6), ((food, great), 5), ((sauce, hot), 5), ((tea, sweet), 5)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rii85bzYKGC9P0zOyAem6A</td>\n",
       "      <td>[((service, great), 15), ((food, great), 12), ((potato, baked), 8), ((place, great), 7), ((rib, prime), 7)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i-2OzvZUDtvKCMq1vcRSZg</td>\n",
       "      <td>[((battleship, super), 10), ((bread, fresh), 10), ((sandwiches, best), 5), ((hoagie, italian), 5), ((hoagies, best), 4)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               BusinessId  \\\n",
       "0  8Z72HW5ydzQFydUxZglurg   \n",
       "1  QeEQXdto_4wFRaNKyIygRA   \n",
       "2  IUMyUYOIR9UQ7XGIEQKOuA   \n",
       "3  Rii85bzYKGC9P0zOyAem6A   \n",
       "4  i-2OzvZUDtvKCMq1vcRSZg   \n",
       "\n",
       "                                                                                                                AdjNounPair  \n",
       "0  [((food, good), 12), ((buffet, chinese), 10), ((bar, sushi), 8), ((staff, friendly), 6), ((buffets, chinese), 6)]         \n",
       "1  [((quarter, french), 29), ((food, good), 15), ((cristo, monte), 11), ((service, great), 8), ((service, good), 6)]         \n",
       "2  [((food, good), 8), ((service, slow), 6), ((food, great), 5), ((sauce, hot), 5), ((tea, sweet), 5)]                       \n",
       "3  [((service, great), 15), ((food, great), 12), ((potato, baked), 8), ((place, great), 7), ((rib, prime), 7)]               \n",
       "4  [((battleship, super), 10), ((bread, fresh), 10), ((sandwiches, best), 5), ((hoagie, italian), 5), ((hoagies, best), 4)]  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import random\n",
    "import sys\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import Counter\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def selectNBusinessId(df, seed=4, numberOfBusineesId=5):\n",
    "\n",
    "    business_id_list = df.business_id.values.tolist()\n",
    "    random.Random(4).shuffle(business_id_list)\n",
    "\n",
    "    df_5 = df[df.business_id.isin(business_id_list[:numberOfBusineesId])]\n",
    "\n",
    "    bs_text_map = dict()\n",
    "    for bid in business_id_list[:numberOfBusineesId]:\n",
    "        bs_text_map[bid] = df_5[df_5['business_id'] == bid].text.tolist()\n",
    "\n",
    "    print('\\n\\n{} Selecting {} business ID {}\\n'.format('-'*20, numberOfBusineesId, '-'*20))\n",
    "\n",
    "    for b in bs_text_map.keys():\n",
    "        print('Business ID: ' ,b)\n",
    "    \n",
    "    return bs_text_map\n",
    "\n",
    "class NounAndAdjPair:\n",
    "    \n",
    "    patterns=['(ADJ )*(NOUN )+(ADV )*(VERB )+(ADV )*ADJ ', # The Korean grill is good\n",
    "              '(NOUN )+.*PRON (VERB )+(ADV )*ADJ ', # I like the food, which is good\n",
    "              '(ADV )*ADJ (NOUN )+', # good service\n",
    "             ]\n",
    "    \n",
    "    def __init__(self, doc, withExtra=False):\n",
    "        \n",
    "        def getDoc():\n",
    "            return re.sub(' +', ' ', self.original_doc.replace('\\n', ' ').strip()).lower()\n",
    "        \n",
    "        def getOriginalSentences():\n",
    "            return [s for s in sent_tokenize(self.doc)] #nltk tokenizer\n",
    "\n",
    "        def getSentences():\n",
    "            return [' '.join([w.text for w in nlp(s)]) for s in self.original_sentences] # spacy word tokenizer\n",
    "\n",
    "        def getTaggings():\n",
    "            return [' '.join([w.pos_ for w in nlp(s)])+' ' for s in self.original_sentences]  \n",
    "        \n",
    "        self.withExtra = withExtra\n",
    "        self.original_doc = doc\n",
    "        self.doc = getDoc()\n",
    "        self.original_sentences = getOriginalSentences()\n",
    "        self.sentences = getSentences()\n",
    "        self.taggings = getTaggings()\n",
    "#         print(len(self.sentences), len(self.taggings))\n",
    "\n",
    "    def getPairsWithFSA(self, returnOnlyPairs=False, returnInDf=False):\n",
    "        \n",
    "        def checkIfAdjAndNounExists(tagging):\n",
    "            if 'ADJ' in tagging and 'NOUN' in tagging:\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def getWoldsByTaggingIndex(target_tagging_index, no_sentence, no_pattern):\n",
    "            \n",
    "            original_sentence = self.sentences[no_sentence]\n",
    "            original_tagging = self.taggings[no_sentence].strip()\n",
    "            \n",
    "            baseIndex = 0 if target_tagging_index[0]==0 else len(original_tagging[:target_tagging_index[0]].strip().split(' '))\n",
    "            buildIndex = len(original_tagging[target_tagging_index[0]:target_tagging_index[1]].strip().split(' '))\n",
    "            \n",
    "            trimmed_tagging_list = original_tagging.strip().split(' ')[baseIndex: baseIndex+buildIndex]\n",
    "            trimmed_sentence_list = original_sentence.split(' ')[baseIndex: baseIndex+buildIndex]\n",
    "            \n",
    "            noun_adj_pair = []\n",
    "            \n",
    "            \n",
    "            if no_pattern==0: # the first pattern\n",
    "                if self.withExtra:\n",
    "                    last_verb_index = len(trimmed_tagging_list)-trimmed_tagging_list[::-1].index('VERB')-1\n",
    "                    for i in range(len(trimmed_tagging_list)):\n",
    "                        if trimmed_tagging_list[i] not in ['ADJ', 'NOUN']: #filter out adv\n",
    "                            break        \n",
    "                    noun_adj_pair.append(' '.join(trimmed_sentence_list[:i]))\n",
    "                    noun_adj_pair.append(' '.join(trimmed_sentence_list[last_verb_index+1:]))\n",
    "                else:\n",
    "                    noun_adj_pair.append(' '.join([trimmed_sentence_list[i] for i in [i for i, t in enumerate(trimmed_tagging_list) if t == \"NOUN\"]]))\n",
    "                    noun_adj_pair.append(trimmed_sentence_list[-1])\n",
    "                    \n",
    "                \n",
    "            elif no_pattern==1:\n",
    "                if self.withExtra:\n",
    "                    last_noun_end_index = len(trimmed_tagging_list)-trimmed_tagging_list[::-1].index('NOUN')-1\n",
    "                    last_noun_start_index = last_noun_end_index\n",
    "                    for i in range(last_noun_end_index, 0, -1):\n",
    "                        if trimmed_tagging_list[i]=='NOUN':\n",
    "                            last_noun_start_index = i\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "                    last_verb_index = len(trimmed_tagging_list)-trimmed_tagging_list[::-1].index('VERB')-1\n",
    "\n",
    "                    noun_adj_pair.append(' '.join(trimmed_sentence_list[last_noun_start_index:last_noun_end_index+1]))\n",
    "                    noun_adj_pair.append(' '.join(trimmed_sentence_list[last_verb_index+1:]))\n",
    "                else:\n",
    "                    noun_index = [i for i, t in enumerate(trimmed_tagging_list) if t == \"NOUN\"][::-1]\n",
    "                    if len(noun_index)==1:\n",
    "                        noun_adj_pair.append(' '.join([trimmed_sentence_list[i] for i in noun_index]))\n",
    "                    else:\n",
    "                        for noun_start_index in range(1, len(noun_index)):\n",
    "                            if noun_index[noun_start_index]!=noun_index[noun_start_index-1]-1:\n",
    "                                break\n",
    "                        noun_adj_pair.append(' '.join([trimmed_sentence_list[i] for i in noun_index[:noun_start_index][::1]]))\n",
    "                    noun_adj_pair.append(trimmed_sentence_list[-1])\n",
    "                    \n",
    "                    \n",
    "            elif no_pattern==2:\n",
    "                if self.withExtra:\n",
    "                    noun_index = trimmed_tagging_list.index('NOUN')\n",
    "                    noun_adj_pair.append(' '.join(trimmed_sentence_list[noun_index:]))\n",
    "                    noun_adj_pair.append(' '.join(trimmed_sentence_list[:noun_index]))\n",
    "                else:\n",
    "                    noun_adj_pair.append(' '.join([trimmed_sentence_list[i] for i in [i for i, t in enumerate(trimmed_tagging_list) if t == \"NOUN\"]]))\n",
    "                    noun_adj_pair.append(trimmed_sentence_list[trimmed_tagging_list.index('ADJ')])\n",
    "                    \n",
    "            \n",
    "            return tuple(noun_adj_pair)\n",
    "        \n",
    "        \n",
    "        def formDf(sentence_adjNoun_pair):\n",
    "            df = pd.DataFrame(columns=['Sentence', 'AdjNounPair'])\n",
    "            pd.set_option('display.max_colwidth', -1)\n",
    "            df.Sentence, df.AdjNounPair = list(sentence_adjNoun_pair.keys()), list(sentence_adjNoun_pair.values())\n",
    "            return df\n",
    "        \n",
    "        sentence_adjNoun_pair = dict(zip(self.sentences, [[] for i in range(len(self.sentences))]))\n",
    "        \n",
    "        for i in range(len(self.sentences)):\n",
    "            s = self.sentences[i]\n",
    "            t = self.taggings[i]\n",
    "            \n",
    "            if checkIfAdjAndNounExists(t):\n",
    "                for p in self.patterns:\n",
    "                    for x in re.finditer(p, t):\n",
    "                        sentence_adjNoun_pair[s].append(getWoldsByTaggingIndex(x.span(), i, self.patterns.index(p)))\n",
    "                        \n",
    "        if returnOnlyPairs:\n",
    "            pairs = []\n",
    "            for p in sentence_adjNoun_pair.values():\n",
    "                pairs.extend(p)\n",
    "            return pairs\n",
    "                        \n",
    "        if returnInDf:\n",
    "            return formDf(sentence_adjNoun_pair)\n",
    "            \n",
    "        return sentence_adjNoun_pair\n",
    "\n",
    "def getPairs(bs_text_map, numberOfPairs=5, returnInDf=True):\n",
    "\n",
    "    bs_pair_map = dict(zip(list(bs_text_map.keys()), [[] for i in range(len(list(bs_text_map.keys())))]))\n",
    "\n",
    "    print('\\n{} Getting pairs {}\\n'.format('-'*25, '-'*25))\n",
    "\n",
    "    for b in list(bs_pair_map.keys()):\n",
    "        print('Processing reviews from business ID: {} ...'.format(b))\n",
    "        docs = bs_text_map[b]\n",
    "        for doc in docs:\n",
    "            doc_nlp = NounAndAdjPair(doc)\n",
    "            bs_pair_map[b].extend(doc_nlp.getPairsWithFSA(returnOnlyPairs=True))\n",
    "        \n",
    "\n",
    "    for bs in list(bs_pair_map.keys()):\n",
    "        keys = list(Counter(bs_pair_map[bs]).keys())# equals to list(set(words))\n",
    "        values = list(Counter(bs_pair_map[bs]).values())\n",
    "        keys_index = sorted(range(len(values)), key=lambda k: values[k], reverse=True)\n",
    "        bs_pair_map[bs] = [(keys[i], values[i]) for i in keys_index[:numberOfPairs]]\n",
    "\n",
    "    if returnInDf:\n",
    "        df = pd.DataFrame(columns=['BusinessId', 'AdjNounPair'])\n",
    "        df.BusinessId, df.AdjNounPair = list(bs_pair_map.keys()), list(bs_pair_map.values())\n",
    "        return df\n",
    "\n",
    "    return bs_pair_map\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "df = pd.read_csv('/Users/heyuhao/Documents/uni/y3s1/original/19S1-CE4045-CZ4045-NATURAL LANG PROCESSING/Content/Assignment/data.csv', index_col=0) # more options can be specified also\n",
    "getPairs(selectNBusinessId(df))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORIGINAL JUPYTER NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import random\n",
    "import sys\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import Counter\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/heyuhao/Documents/uni/y3s1/original/19S1-CE4045-CZ4045-NATURAL LANG PROCESSING/Content/Assignment/data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8aoJJdKEO3ypoZNszpPu7Q</td>\n",
       "      <td>bGgAL09pxLnV_FFgR4ZADg</td>\n",
       "      <td>ZBE-H_aUlicix_9vUGQPIQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>We had my Mother's Birthday Party here on 10/2...</td>\n",
       "      <td>2016-11-09 20:07:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J5NOCLdhuhor7USRhtYZ8w</td>\n",
       "      <td>pFCb-1j6oI3TDjr26h2cJQ</td>\n",
       "      <td>e-YnECeZNt8ngm0tu4X9mQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Good Korean grill near Eaton Centre. The marin...</td>\n",
       "      <td>2015-12-05 05:06:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PXiLWAYRt3xnHaJ8MB4rzw</td>\n",
       "      <td>mEzc6LeTNiQgIVsq3poMbg</td>\n",
       "      <td>j7HO1YeMQGYo3KibMXZ5vg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Was recommended to try this place by few peopl...</td>\n",
       "      <td>2014-10-11 05:16:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VrLarvxZYJm74yAqtpe9PQ</td>\n",
       "      <td>o-zUN2WEZgjQS7jnNsec0g</td>\n",
       "      <td>7e3PZzUpG5FYOTGt3O3ePA</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ambience: Would not expect something this nice...</td>\n",
       "      <td>2016-07-25 03:45:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1CUpidlVFprUCkApqzCmA</td>\n",
       "      <td>Wlx0iBXJvk4x0EeOt2Bz1Q</td>\n",
       "      <td>vuHzLZ7nAeT-EiecOkS5Og</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Absolutely the WORST pool company that I have ...</td>\n",
       "      <td>2016-04-11 18:49:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  8aoJJdKEO3ypoZNszpPu7Q  bGgAL09pxLnV_FFgR4ZADg  ZBE-H_aUlicix_9vUGQPIQ   \n",
       "1  J5NOCLdhuhor7USRhtYZ8w  pFCb-1j6oI3TDjr26h2cJQ  e-YnECeZNt8ngm0tu4X9mQ   \n",
       "2  PXiLWAYRt3xnHaJ8MB4rzw  mEzc6LeTNiQgIVsq3poMbg  j7HO1YeMQGYo3KibMXZ5vg   \n",
       "3  VrLarvxZYJm74yAqtpe9PQ  o-zUN2WEZgjQS7jnNsec0g  7e3PZzUpG5FYOTGt3O3ePA   \n",
       "4  C1CUpidlVFprUCkApqzCmA  Wlx0iBXJvk4x0EeOt2Bz1Q  vuHzLZ7nAeT-EiecOkS5Og   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0    5.0       0      0     0   \n",
       "1    4.0       0      0     0   \n",
       "2    5.0       2      1     3   \n",
       "3    3.0       0      0     0   \n",
       "4    1.0      11      0     3   \n",
       "\n",
       "                                                text                 date  \n",
       "0  We had my Mother's Birthday Party here on 10/2...  2016-11-09 20:07:25  \n",
       "1  Good Korean grill near Eaton Centre. The marin...  2015-12-05 05:06:43  \n",
       "2  Was recommended to try this place by few peopl...  2014-10-11 05:16:15  \n",
       "3  Ambience: Would not expect something this nice...  2016-07-25 03:45:26  \n",
       "4  Absolutely the WORST pool company that I have ...  2016-04-11 18:49:11  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "business = df.groupby('business_id').business_id.count().to_list()\n",
    "\n",
    "Counter(business).keys(), Counter(business).values()\n",
    "\n",
    "# As the result says, there are 153 unique business id, and 100 review for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_id_list = df.business_id.values.tolist()\n",
    "\n",
    "random.Random(4).shuffle(business_id_list)\n",
    "\n",
    "business_id_list_5 = business_id_list[:5]\n",
    "print(business_id_list_5, '\\n\\n')\n",
    "\n",
    "df_5 = df[df.business_id.isin(business_id_list_5)]\n",
    "\n",
    "bs_text_map = dict()\n",
    "for bid in business_id_list_5:\n",
    "    bs_text_map[bid] = df_5[df_5['business_id'] == bid].text.tolist()\n",
    "\n",
    "for i in bs_text_map.items():\n",
    "    print(i[0], ', number of reviews:', len(i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keep punctuation: The service is fast, room is clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NounAndAdjPair:\n",
    "    \n",
    "    patterns=['(ADJ )*(NOUN )+(ADV )*(VERB )+(ADV )*ADJ ', # The Korean grill is good\n",
    "              '(NOUN )+.*PRON (VERB )+(ADV )*ADJ ', \n",
    "              '(ADV )*ADJ (NOUN )+', # good service\n",
    "             ]\n",
    "    \n",
    "    def __init__(self, doc, withExtra=False):\n",
    "        \n",
    "        def getDoc():\n",
    "            return re.sub(' +', ' ', self.original_doc.replace('\\n', ' ').strip()).lower()\n",
    "        \n",
    "        def getOriginalSentences():\n",
    "            return [s for s in sent_tokenize(self.doc)] #nltk tokenizer\n",
    "\n",
    "        def getSentences():\n",
    "            return [' '.join([w.text for w in nlp(s)]) for s in self.original_sentences] # spacy word tokenizer\n",
    "\n",
    "        def getTaggings():\n",
    "            return [' '.join([w.pos_ for w in nlp(s)])+' ' for s in self.original_sentences]  \n",
    "        \n",
    "        self.withExtra = withExtra\n",
    "        self.original_doc = doc\n",
    "        self.doc = getDoc()\n",
    "        self.original_sentences = getOriginalSentences()\n",
    "        self.sentences = getSentences()\n",
    "        self.taggings = getTaggings()\n",
    "#         print(len(self.sentences), len(self.taggings))\n",
    "\n",
    "    def getPairsWithFSA(self, returnOnlyPairs=False, returnInDf=False):\n",
    "        \n",
    "        def checkIfAdjAndNounExists(tagging):\n",
    "            if 'ADJ' in tagging and 'NOUN' in tagging:\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def getWoldsByTaggingIndex(target_tagging_index, no_sentence, no_pattern):\n",
    "            \n",
    "            original_sentence = self.sentences[no_sentence]\n",
    "            original_tagging = self.taggings[no_sentence].strip()\n",
    "            \n",
    "            baseIndex = 0 if target_tagging_index[0]==0 else len(original_tagging[:target_tagging_index[0]].strip().split(' '))\n",
    "            buildIndex = len(original_tagging[target_tagging_index[0]:target_tagging_index[1]].strip().split(' '))\n",
    "            \n",
    "            trimmed_tagging_list = original_tagging.strip().split(' ')[baseIndex: baseIndex+buildIndex]\n",
    "            trimmed_sentence_list = original_sentence.split(' ')[baseIndex: baseIndex+buildIndex]\n",
    "            \n",
    "            noun_adj_pair = []\n",
    "            \n",
    "            \n",
    "            if no_pattern==0: # the first pattern\n",
    "                if self.withExtra:\n",
    "                    last_verb_index = len(trimmed_tagging_list)-trimmed_tagging_list[::-1].index('VERB')-1\n",
    "                    for i in range(len(trimmed_tagging_list)):\n",
    "                        if trimmed_tagging_list[i] not in ['ADJ', 'NOUN']: #filter out adv\n",
    "                            break        \n",
    "                    noun_adj_pair.append(' '.join(trimmed_sentence_list[:i]))\n",
    "                    noun_adj_pair.append(' '.join(trimmed_sentence_list[last_verb_index+1:]))\n",
    "                else:\n",
    "                    noun_adj_pair.append(' '.join([trimmed_sentence_list[i] for i in [i for i, t in enumerate(trimmed_tagging_list) if t == \"NOUN\"]]))\n",
    "                    noun_adj_pair.append(trimmed_sentence_list[-1])\n",
    "                    \n",
    "                \n",
    "            elif no_pattern==1:\n",
    "                if self.withExtra:\n",
    "                    last_noun_end_index = len(trimmed_tagging_list)-trimmed_tagging_list[::-1].index('NOUN')-1\n",
    "                    last_noun_start_index = last_noun_end_index\n",
    "                    for i in range(last_noun_end_index, 0, -1):\n",
    "                        if trimmed_tagging_list[i]=='NOUN':\n",
    "                            last_noun_start_index = i\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "                    last_verb_index = len(trimmed_tagging_list)-trimmed_tagging_list[::-1].index('VERB')-1\n",
    "\n",
    "                    noun_adj_pair.append(' '.join(trimmed_sentence_list[last_noun_start_index:last_noun_end_index+1]))\n",
    "                    noun_adj_pair.append(' '.join(trimmed_sentence_list[last_verb_index+1:]))\n",
    "                else:\n",
    "                    noun_index = [i for i, t in enumerate(trimmed_tagging_list) if t == \"NOUN\"][::-1]\n",
    "                    if len(noun_index)==1:\n",
    "                        noun_adj_pair.append(' '.join([trimmed_sentence_list[i] for i in noun_index]))\n",
    "                    else:\n",
    "                        for noun_start_index in range(1, len(noun_index)):\n",
    "                            if noun_index[noun_start_index]!=noun_index[noun_start_index-1]-1:\n",
    "                                break\n",
    "                        noun_adj_pair.append(' '.join([trimmed_sentence_list[i] for i in noun_index[:noun_start_index][::1]]))\n",
    "                    noun_adj_pair.append(trimmed_sentence_list[-1])\n",
    "                    \n",
    "                    \n",
    "            elif no_pattern==2:\n",
    "                if self.withExtra:\n",
    "                    noun_index = trimmed_tagging_list.index('NOUN')\n",
    "                    noun_adj_pair.append(' '.join(trimmed_sentence_list[noun_index:]))\n",
    "                    noun_adj_pair.append(' '.join(trimmed_sentence_list[:noun_index]))\n",
    "                else:\n",
    "                    noun_adj_pair.append(' '.join([trimmed_sentence_list[i] for i in [i for i, t in enumerate(trimmed_tagging_list) if t == \"NOUN\"]]))\n",
    "                    noun_adj_pair.append(trimmed_sentence_list[trimmed_tagging_list.index('ADJ')])\n",
    "                    \n",
    "            \n",
    "            return tuple(noun_adj_pair)\n",
    "        \n",
    "        \n",
    "        def formDf(sentence_adjNoun_pair):\n",
    "            df = pd.DataFrame(columns=['Sentence', 'AdjNounPair'])\n",
    "            pd.set_option('display.max_colwidth', -1)\n",
    "            df.Sentence, df.AdjNounPair = list(sentence_adjNoun_pair.keys()), list(sentence_adjNoun_pair.values())\n",
    "            return df\n",
    "        \n",
    "        sentence_adjNoun_pair = dict(zip(self.sentences, [[] for i in range(len(self.sentences))]))\n",
    "        \n",
    "        for i in range(len(self.sentences)):\n",
    "            s = self.sentences[i]\n",
    "            t = self.taggings[i]\n",
    "            \n",
    "            if checkIfAdjAndNounExists(t):\n",
    "                for p in self.patterns:\n",
    "                    for x in re.finditer(p, t):\n",
    "                        sentence_adjNoun_pair[s].append(getWoldsByTaggingIndex(x.span(), i, self.patterns.index(p)))\n",
    "                        \n",
    "        if returnOnlyPairs:\n",
    "            pairs = []\n",
    "            for p in sentence_adjNoun_pair.values():\n",
    "                pairs.extend(p)\n",
    "            return pairs\n",
    "                        \n",
    "        if returnInDf:\n",
    "            return formDf(sentence_adjNoun_pair)\n",
    "            \n",
    "        return sentence_adjNoun_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=NounAndAdjPair(df.text[2])\n",
    "temp.getPairsWithFSA(returnInDf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_pair_map = dict(zip(business_id_list_5, [[] for i in business_id_list_5]))\n",
    "\n",
    "for b in bs_pair_map.keys():\n",
    "    docs = bs_text_map[b]\n",
    "    for doc in docs:\n",
    "        doc_nlp = NounAndAdjPair(doc)\n",
    "        bs_pair_map[b].extend(doc_nlp.getPairsWithFSA(returnOnlyPairs=True))\n",
    "\n",
    "bs_pair_map_5 = dict(zip(business_id_list_5, [[] for i in business_id_list_5]))\n",
    "\n",
    "for bs in list(bs_pair_map.keys()):\n",
    "    keys = list(Counter(bs_pair_map[bs]).keys())# equals to list(set(words))\n",
    "    values = list(Counter(bs_pair_map[bs]).values())\n",
    "    keys_index = sorted(range(len(values)), key=lambda k: values[k], reverse=True)\n",
    "    bs_pair_map_5[bs] = [(keys[i], values[i]) for i in keys_index[:10]]\n",
    "    \n",
    "bs_pair_map_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
